denialyzer.py-

import sys
import os
import re
import subprocess
import sys
import fnmatch

with_ts_dict = {}
def process_file(filename):
    f = open(filename, encoding='utf8', errors='ignore')
    for line in f:
        match = re.match(r'.*{(.*)}.* scontext=([^\s]+) tcontext=([^\s]+) tclass=([^\s]+) ',line)
    #value = match.findall(line)
    #for val in value:
    #    print(val)
        if match is not None:
            first_id = match.group(1)
            second_id = match.group(2)
            third_id = match.group(3)
            fourth_id = match.group(4)
            #print(first_id.strip(),second_id,third_id,fourth_id)
            denial_detail_list = with_ts_dict.get((first_id,second_id,third_id,fourth_id))
            if denial_detail_list is None:
                denial_detail_list=[]
                denial_detail_list.append(filename)
                denial_detail_list.append(line)
                with_ts_dict[(first_id,second_id,third_id,fourth_id)]=denial_detail_list;
            else :
                denial_detail_list.append(line)
                with_ts_dict[(first_id,second_id,third_id,fourth_id)]=denial_detail_list;
            #print(line)


def find_files(search_path):
    with_ts_dict.clear()
    print("searching in ", search_path)

    # indexing the directory to find the requested file
    for root, dir, files in os.walk(search_path):
        # print(files)
        for f in files:
            if fnmatch.fnmatch(f, '*.log'):
                # print(file)
                # print(os.path.join(root,file))
                process_file(os.path.join(root,f))

def find_dlt_files(search_path):
    with_ts_dict.clear()
    print("searching in ", search_path)

    # indexing the directory to find the requested file
    for root, dir, files in os.walk(search_path):
        # print(files)
        for f in files:
            if fnmatch.fnmatch(f, '*.dlt'):
                # print(file)
                # print(oso.path.join(root,file))
                logfile = os.path.join(root,f) + '.log'
                #logfile,ext = os.path.splitext(os.path.join(root,f))
                #logfile = logfile + '.log'
                dltfile = os.path.join(root,f)
                #dlt-viewer -s -u  -c ~/workspace/ecockpit/logs/DLT_logs/test_denials.dlt  test_denials.dlt.log
                cmd = "dlt-viewer -s -u -c %s %s" % (dltfile,logfile)
                #cmd = ['dlt-viewer', '-c' dltfile, logfile]
                procId = subprocess.Popen(cmd.split(), stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                procId.wait()

def usage():
    print("python3 denialyzer.py /home/prasadt1/workspace/ecockpit/selinux/SElinuxDLTLogs false")
    print("python3 denialyzer.py /home/prasadt1/workspace/ecockpit/selinux/SElinuxDLTLogs true")
    sys.exit()

if __name__ == '__main__':
    #find_dlt_files(sys.argv[1])
    if len(sys.argv) < 3:
        print("Enter the command line arguments ")
        usage()
    verbose = sys.argv[2].lower() == 'true'
    find_files(sys.argv[1])
    for key, value in with_ts_dict.items():
        #print(key[0],key[1],key[2],key[3])
        print("{} scontext={} tcontext={} tclass={}".format(key[0],key[1],key[2],key[3]))
        if verbose:
            print('***********************************************')
            for val in value:
                print(val)
            print('-----------------------------------------------')
    print("Total Unique Denials : {}".format(len(with_ts_dict)))


Explaination-

This Python script is designed to analyze SELinux denial logs, particularly those stored in DLT (Diagnostic Log and Trace) files. 
Here's a breakdown of what the code does:

Overview

The script processes log files (both .log and .dlt formats) containing SELinux denial messages, extracts relevant information, 
and organizes them for analysis.

Key Components

1. Global Dictionary

with_ts_dict = {}

Stores processed denial entries with their contexts as keys and matching log lines as values
2. process_file() Function

Processes individual log files to extract SELinux denial information:

Opens the file with UTF-8 encoding (ignoring errors)
Uses regex to match lines containing SELinux denial patterns:

r'.*{(.*)}.* scontext=([^\s]+) tcontext=([^\s]+) tclass=([^\s]+) '

Captures:
Audit ID (in curly braces)
Source context (scontext)
Target context (tcontext)
Target class (tclass)
Stores matches in with_ts_dict with:
Key: Tuple of (audit ID, scontext, tcontext, tclass)
Value: List of log lines containing this pattern
3. find_files() Function

Searches for .log files in a directory tree:

Clears the global dictionary
Walks through all directories starting from search_path
Processes each .log file found

4. find_dlt_files() Function

Handles DLT files specifically:

Clears the global dictionary
Finds all .dlt files in the directory tree
For each DLT file:
Creates a corresponding .log filename
Uses dlt-viewer command-line tool to convert DLT to log format
Executes the conversion via subprocess
5. Main Execution

Checks command-line arguments:
Requires a path and a boolean ('true'/'false') for verbose output


Calls find_files() to process logs
Prints results:
Summary of unique denials (without details if not verbose)
Full log lines for each denial if verbose mode is enabled
Reports total count of unique denials
Usage Examples

# Basic usage (non-verbose)
python3 denialyzer.py /path/to/logs false

# Verbose output
python3 denialyzer.py /path/to/logs true

Key Features

SELinux Specific Parsing: Focuses on extracting SELinux denial contexts
DLT File Support: Can process binary DLT files by converting them to text logs
Duplicate Handling: Groups identical denials together
Flexible Output: Can show either summaries or detailed logs
Limitations/Notes

Requires dlt-viewer tool to be available in PATH for DLT file processing
The regex pattern is specific to a particular SELinux log format
Error handling is minimal - assumes files are readable and formats are correct
The DLT conversion creates temporary .log files alongside the original DLT files
The script appears to be part of a larger SELinux denial analysis toolchain, likely used for debugging or security auditing purposes.


============================================================================================================================================================================================================================

gzip_extract_logs_v2.py-

import os
import sys
import gzip
import re

def find_gz_files(directory):
    """
    Recursively search the given directory for all .gz files.

    Args:
        directory (str): Path to the directory to search.

    Returns:
        list: List of paths to .gz files.
    """
    gz_files = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".gz"):
                gz_files.append(os.path.join(root, file))
    return gz_files

def search_pattern_in_gz_files(gz_files, pattern):
    """
    Search for a pattern in the given .gz files.

    Args:
        gz_files (list): List of .gz file paths.
        pattern (str): Regex pattern to search for.

    Returns:
        list: List of .gz files containing the pattern.
    """
    matching_files = []
    regex = re.compile(pattern)
    for gz_file in gz_files:
        try:
            with gzip.open(gz_file, 'rt', errors="replace") as f:
                for line in f:
                    if regex.search(line):
                        matching_files.append(gz_file)
                        break
        except Exception as e:
            print(f"Reading {gz_file}: Done")
    return matching_files

def extract_gz_files(gz_files):
    """
    Extract the given .gz files to their respective directories with the same name.

    Args:
        gz_files (list): List of .gz file paths.
    """
    for gz_file in gz_files:
        try:
            output_file = gz_file[:-3]  # Remove the .gz extension
            with gzip.open(gz_file, 'rb') as f_in, open(output_file, 'wb') as f_out:
                chunk = f_in.read(1024)  # Read the first chunk
                while chunk:  # Continue until no more data
                    f_out.write(chunk)
                    chunk = f_in.read(1024)
            print(f"Extracted {gz_file} to {output_file}")
        except OSError as e:
            #print(f"Error extracting {gz_file}: {e}")
            if "end-of-stream marker" in str(e):
                print(f"Warning: {gz_file} appears to be incomplete or corrupted.")
        except Exception as e:
            #print(f"Unexpected error with {gz_file}: {e}")
            print(f"Extracting File {gz_file} done")

# Main Program
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python3 gzip_extract_logs_v2.py <path to logs> <time of issue>")
        sys.exit(1)

    directory = sys.argv[1]
    pattern = sys.argv[2]

    # Step 1: Find all .gz files
    gz_files = find_gz_files(directory)
    print(f"Found {len(gz_files)} .gz files.")

    # Step 2: Search for the pattern in .gz files
    matching_files = search_pattern_in_gz_files(gz_files, pattern)
    print(f"Found {len(matching_files)} files matching the pattern.")

    # Step 3: Extract matching .gz files
    if matching_files:
        extract_gz_files(matching_files)
    else:
        print("No matching files to extract.")


Explaination-

This Python script is designed to search for and extract compressed .gz log files that contain a specific pattern (typically related to a time of issue or other search term). Here's a detailed explanation:

1. Key Functions

find_gz_files(directory)

Purpose: Recursively searches a directory for .gz files.
How it works:
Uses os.walk() to traverse all subdirectories.
Checks if a file ends with .gz and collects its full path.
Returns: A list of .gz file paths.

search_pattern_in_gz_files(gz_files, pattern)

Purpose: Searches inside .gz files for a given regex pattern.
How it works:
Opens each .gz file in text mode ('rt') with error handling.
Uses re.compile() to compile the regex pattern.
If a match is found, the file is added to matching_files.
Returns: A list of .gz files containing the pattern.


extract_gz_files(gz_files)

Purpose: Extracts .gz files to their original (uncompressed) form.
How it works:
Removes the .gz extension from the filename.
Reads the compressed file in chunks (1024 bytes at a time) and writes them to the output file.
Handles incomplete/corrupted files gracefully.
Output: Prints extraction status or errors.

2. Main Program Flow

Command-Line Arguments Check:
Requires two arguments:
<path to logs>: Directory to search for .gz files.
<time of issue>: Regex pattern to search for (e.g., "2023-10-01 12:00").
If arguments are missing, prints usage and exits.
Step 1: Find .gz Files
Calls find_gz_files() to get all .gz files in the directory.
Step 2: Search for the Pattern
Calls search_pattern_in_gz_files() to filter files containing the pattern.
Step 3: Extract Matching Files
If matches are found, extracts them using extract_gz_files().
If no matches, prints a message.

3. Key Features

✅ Recursive Search: Finds .gz files in all subdirectories.
✅ Efficient Pattern Matching: Uses regex to search inside compressed files without full extraction.
✅ Chunk-Based Extraction: Avoids memory issues by reading/writing in 1024-byte chunks.
✅ Error Handling:

Skips corrupted/incomplete .gz files (e.g., missing end-of-stream marker).
Uses errors="replace" to handle encoding issues.
✅ Progress Feedback: Prints status updates (e.g., "Extracted X to Y").

4. Example Usage

python3 gzip_extract_logs_v2.py /var/log/ "2023-10-01 12:00"

Searches: All .gz files under /var/log/ for the timestamp "2023-10-01 12:00".
Extracts: Only the matching files (e.g., syslog.1.gz → syslog.1).
5. Possible Improvements

Parallel Processing: Speed up searching/extracting using multiprocessing.
Better Error Logging: Log errors to a file instead of just printing.
Dry Run Mode: Add a --dry-run flag to preview matches without extraction.
Case-Insensitive Search: Modify regex to ignore case (e.g., re.IGNORECASE).

Summary

This script is useful for quickly finding and extracting relevant log entries from compressed files, particularly in 
debugging scenarios where logs are archived in .gz format. It efficiently filters files before extraction, saving disk
space and processing time.
================================================================================================================================================================================================================================

Readme-
This script does the following:
1.Convert the provided DLT logs to log format
2.Extract all the unique denials 
3.Publish the denials in both verbose and non-verbose modes
===================================================================================================================================================================================================

dlt-viewer-old -s -u -c 2024-12-11_10-05-36_2024-12-11_10-07-36_INK5_TA10121_X12345_BN_IUKETH_000024_MGU_02_UDP_000_x2eNextC.dlt logd.txt
 
Current DST offset: 0 Current TimeZoneID: 0 Current UTC offset: 28800
 

