denialyzer.py-

import sys
import os
import re
import subprocess
import sys
import fnmatch

with_ts_dict = {}
def process_file(filename):
    f = open(filename, encoding='utf8', errors='ignore')
    for line in f:
        match = re.match(r'.*{(.*)}.* scontext=([^\s]+) tcontext=([^\s]+) tclass=([^\s]+) ',line)
    #value = match.findall(line)
    #for val in value:
    #    print(val)
        if match is not None:
            first_id = match.group(1)
            second_id = match.group(2)
            third_id = match.group(3)
            fourth_id = match.group(4)
            #print(first_id.strip(),second_id,third_id,fourth_id)
            denial_detail_list = with_ts_dict.get((first_id,second_id,third_id,fourth_id))
            if denial_detail_list is None:
                denial_detail_list=[]
                denial_detail_list.append(filename)
                denial_detail_list.append(line)
                with_ts_dict[(first_id,second_id,third_id,fourth_id)]=denial_detail_list;
            else :
                denial_detail_list.append(line)
                with_ts_dict[(first_id,second_id,third_id,fourth_id)]=denial_detail_list;
            #print(line)


def find_files(search_path):
    with_ts_dict.clear()
    print("searching in ", search_path)

    # indexing the directory to find the requested file
    for root, dir, files in os.walk(search_path):
        # print(files)
        for f in files:
            if fnmatch.fnmatch(f, '*.log'):
                # print(file)
                # print(os.path.join(root,file))
                process_file(os.path.join(root,f))

def find_dlt_files(search_path):
    with_ts_dict.clear()
    print("searching in ", search_path)

    # indexing the directory to find the requested file
    for root, dir, files in os.walk(search_path):
        # print(files)
        for f in files:
            if fnmatch.fnmatch(f, '*.dlt'):
                # print(file)
                # print(oso.path.join(root,file))
                logfile = os.path.join(root,f) + '.log'
                #logfile,ext = os.path.splitext(os.path.join(root,f))
                #logfile = logfile + '.log'
                dltfile = os.path.join(root,f)
                #dlt-viewer -s -u  -c ~/workspace/ecockpit/logs/DLT_logs/test_denials.dlt  test_denials.dlt.log
                cmd = "dlt-viewer -s -u -c %s %s" % (dltfile,logfile)
                #cmd = ['dlt-viewer', '-c' dltfile, logfile]
                procId = subprocess.Popen(cmd.split(), stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                procId.wait()

def usage():
    print("python3 denialyzer.py /home/prasadt1/workspace/ecockpit/selinux/SElinuxDLTLogs false")
    print("python3 denialyzer.py /home/prasadt1/workspace/ecockpit/selinux/SElinuxDLTLogs true")
    sys.exit()

if __name__ == '__main__':
    #find_dlt_files(sys.argv[1])
    if len(sys.argv) < 3:
        print("Enter the command line arguments ")
        usage()
    verbose = sys.argv[2].lower() == 'true'
    find_files(sys.argv[1])
    for key, value in with_ts_dict.items():
        #print(key[0],key[1],key[2],key[3])
        print("{} scontext={} tcontext={} tclass={}".format(key[0],key[1],key[2],key[3]))
        if verbose:
            print('***********************************************')
            for val in value:
                print(val)
            print('-----------------------------------------------')
    print("Total Unique Denials : {}".format(len(with_ts_dict)))


Explaination-

This Python script is designed to analyze SELinux denial logs, particularly those stored in DLT (Diagnostic Log and Trace) files. 
Here's a breakdown of what the code does:

Overview

The script processes log files (both .log and .dlt formats) containing SELinux denial messages, extracts relevant information, 
and organizes them for analysis.

Key Components

1. Global Dictionary

with_ts_dict = {}

Stores processed denial entries with their contexts as keys and matching log lines as values
2. process_file() Function

Processes individual log files to extract SELinux denial information:

Opens the file with UTF-8 encoding (ignoring errors)
Uses regex to match lines containing SELinux denial patterns:

r'.*{(.*)}.* scontext=([^\s]+) tcontext=([^\s]+) tclass=([^\s]+) '

Captures:
Audit ID (in curly braces)
Source context (scontext)
Target context (tcontext)
Target class (tclass)
Stores matches in with_ts_dict with:
Key: Tuple of (audit ID, scontext, tcontext, tclass)
Value: List of log lines containing this pattern
3. find_files() Function

Searches for .log files in a directory tree:

Clears the global dictionary
Walks through all directories starting from search_path
Processes each .log file found

4. find_dlt_files() Function

Handles DLT files specifically:

Clears the global dictionary
Finds all .dlt files in the directory tree
For each DLT file:
Creates a corresponding .log filename
Uses dlt-viewer command-line tool to convert DLT to log format
Executes the conversion via subprocess
5. Main Execution

Checks command-line arguments:
Requires a path and a boolean ('true'/'false') for verbose output


Calls find_files() to process logs
Prints results:
Summary of unique denials (without details if not verbose)
Full log lines for each denial if verbose mode is enabled
Reports total count of unique denials
Usage Examples

# Basic usage (non-verbose)
python3 denialyzer.py /path/to/logs false

# Verbose output
python3 denialyzer.py /path/to/logs true

Key Features

SELinux Specific Parsing: Focuses on extracting SELinux denial contexts
DLT File Support: Can process binary DLT files by converting them to text logs
Duplicate Handling: Groups identical denials together
Flexible Output: Can show either summaries or detailed logs
Limitations/Notes

Requires dlt-viewer tool to be available in PATH for DLT file processing
The regex pattern is specific to a particular SELinux log format
Error handling is minimal - assumes files are readable and formats are correct
The DLT conversion creates temporary .log files alongside the original DLT files
The script appears to be part of a larger SELinux denial analysis toolchain, likely used for debugging or security auditing purposes.


============================================================================================================================================================================================================================

gzip_extract_logs_v2.py-

import os
import sys
import gzip
import re

def find_gz_files(directory):
    """
    Recursively search the given directory for all .gz files.

    Args:
        directory (str): Path to the directory to search.

    Returns:
        list: List of paths to .gz files.
    """
    gz_files = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".gz"):
                gz_files.append(os.path.join(root, file))
    return gz_files

def search_pattern_in_gz_files(gz_files, pattern):
    """
    Search for a pattern in the given .gz files.

    Args:
        gz_files (list): List of .gz file paths.
        pattern (str): Regex pattern to search for.

    Returns:
        list: List of .gz files containing the pattern.
    """
    matching_files = []
    regex = re.compile(pattern)
    for gz_file in gz_files:
        try:
            with gzip.open(gz_file, 'rt', errors="replace") as f:
                for line in f:
                    if regex.search(line):
                        matching_files.append(gz_file)
                        break
        except Exception as e:
            print(f"Reading {gz_file}: Done")
    return matching_files

def extract_gz_files(gz_files):
    """
    Extract the given .gz files to their respective directories with the same name.

    Args:
        gz_files (list): List of .gz file paths.
    """
    for gz_file in gz_files:
        try:
            output_file = gz_file[:-3]  # Remove the .gz extension
            with gzip.open(gz_file, 'rb') as f_in, open(output_file, 'wb') as f_out:
                chunk = f_in.read(1024)  # Read the first chunk
                while chunk:  # Continue until no more data
                    f_out.write(chunk)
                    chunk = f_in.read(1024)
            print(f"Extracted {gz_file} to {output_file}")
        except OSError as e:
            #print(f"Error extracting {gz_file}: {e}")
            if "end-of-stream marker" in str(e):
                print(f"Warning: {gz_file} appears to be incomplete or corrupted.")
        except Exception as e:
            #print(f"Unexpected error with {gz_file}: {e}")
            print(f"Extracting File {gz_file} done")

# Main Program
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python3 gzip_extract_logs_v2.py <path to logs> <time of issue>")
        sys.exit(1)

    directory = sys.argv[1]
    pattern = sys.argv[2]

    # Step 1: Find all .gz files
    gz_files = find_gz_files(directory)
    print(f"Found {len(gz_files)} .gz files.")

    # Step 2: Search for the pattern in .gz files
    matching_files = search_pattern_in_gz_files(gz_files, pattern)
    print(f"Found {len(matching_files)} files matching the pattern.")

    # Step 3: Extract matching .gz files
    if matching_files:
        extract_gz_files(matching_files)
    else:
        print("No matching files to extract.")


Explaination-

This Python script is designed to search for and extract compressed .gz log files that contain a specific pattern (typically related to a time of issue or other search term). Here's a detailed explanation:

1. Key Functions

find_gz_files(directory)

Purpose: Recursively searches a directory for .gz files.
How it works:
Uses os.walk() to traverse all subdirectories.
Checks if a file ends with .gz and collects its full path.
Returns: A list of .gz file paths.

search_pattern_in_gz_files(gz_files, pattern)

Purpose: Searches inside .gz files for a given regex pattern.
How it works:
Opens each .gz file in text mode ('rt') with error handling.
Uses re.compile() to compile the regex pattern.
If a match is found, the file is added to matching_files.
Returns: A list of .gz files containing the pattern.


extract_gz_files(gz_files)

Purpose: Extracts .gz files to their original (uncompressed) form.
How it works:
Removes the .gz extension from the filename.
Reads the compressed file in chunks (1024 bytes at a time) and writes them to the output file.
Handles incomplete/corrupted files gracefully.
Output: Prints extraction status or errors.

2. Main Program Flow

Command-Line Arguments Check:
Requires two arguments:
<path to logs>: Directory to search for .gz files.
<time of issue>: Regex pattern to search for (e.g., "2023-10-01 12:00").
If arguments are missing, prints usage and exits.
Step 1: Find .gz Files
Calls find_gz_files() to get all .gz files in the directory.
Step 2: Search for the Pattern
Calls search_pattern_in_gz_files() to filter files containing the pattern.
Step 3: Extract Matching Files
If matches are found, extracts them using extract_gz_files().
If no matches, prints a message.

3. Key Features

✅ Recursive Search: Finds .gz files in all subdirectories.
✅ Efficient Pattern Matching: Uses regex to search inside compressed files without full extraction.
✅ Chunk-Based Extraction: Avoids memory issues by reading/writing in 1024-byte chunks.
✅ Error Handling:

Skips corrupted/incomplete .gz files (e.g., missing end-of-stream marker).
Uses errors="replace" to handle encoding issues.
✅ Progress Feedback: Prints status updates (e.g., "Extracted X to Y").

4. Example Usage

python3 gzip_extract_logs_v2.py /var/log/ "2023-10-01 12:00"

Searches: All .gz files under /var/log/ for the timestamp "2023-10-01 12:00".
Extracts: Only the matching files (e.g., syslog.1.gz → syslog.1).
5. Possible Improvements

Parallel Processing: Speed up searching/extracting using multiprocessing.
Better Error Logging: Log errors to a file instead of just printing.
Dry Run Mode: Add a --dry-run flag to preview matches without extraction.
Case-Insensitive Search: Modify regex to ignore case (e.g., re.IGNORECASE).

Summary

This script is useful for quickly finding and extracting relevant log entries from compressed files, particularly in 
debugging scenarios where logs are archived in .gz format. It efficiently filters files before extraction, saving disk
space and processing time.
================================================================================================================================================================================================================================

Readme-
This script does the following:
1.Convert the provided DLT logs to log format
2.Extract all the unique denials 
3.Publish the denials in both verbose and non-verbose modes
===================================================================================================================================================================================================

dlt-viewer-old -s -u -c 2024-12-11_10-05-36_2024-12-11_10-07-36_INK5_TA10121_X12345_BN_IUKETH_000024_MGU_02_UDP_000_x2eNextC.dlt logd.txt
 
Current DST offset: 0 Current TimeZoneID: 0 Current UTC offset: 28800

For extraction of this time below code will work-


import sys
import os
import re
import subprocess
import fnmatch

def extract_utc_offset(line):
    """Extract UTC offset value from a line if present"""
    match = re.search(r'Current UTC offset:\s*(\d+)', line)
    return match.group(1) if match else None

def process_file(filename, results):
    """Process a file to extract UTC offsets"""
    try:
        with open(filename, encoding='utf8', errors='ignore') as f:
            for line in f:
                offset = extract_utc_offset(line)
                if offset:
                    results.setdefault(offset, set()).add(filename)
    except Exception as e:
        print(f"Error processing {filename}: {str(e)}")

def convert_dlt_to_log(dlt_file):
    """Convert DLT file to log format using dlt-viewer"""
    logfile = dlt_file + '.log'
    cmd = f"dlt-viewer -s -u -c {dlt_file} {logfile}"
    
    try:
        proc = subprocess.Popen(cmd.split(), 
                              stdin=subprocess.PIPE,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)
        proc.wait()
        return logfile if proc.returncode == 0 else None
    except Exception as e:
        print(f"Error converting {dlt_file}: {str(e)}")
        return None

def scan_directory(search_path, convert_dlt=False):
    """Scan directory for files and extract UTC offsets"""
    results = {}
    
    for root, _, files in os.walk(search_path):
        for filename in files:
            filepath = os.path.join(root, filename)
            
            if fnmatch.fnmatch(filename, '*.log'):
                process_file(filepath, results)
            elif convert_dlt and fnmatch.fnmatch(filename, '*.dlt'):
                logfile = convert_dlt_to_log(filepath)
                if logfile and os.path.exists(logfile):
                    process_file(logfile, results)
                    # Optional: Remove the temporary log file
                    # os.remove(logfile)
    
    return results

def print_results(results):
    """Print the collected UTC offset results"""
    if not results:
        print("No UTC offset values found in the scanned files.")
        return
    
    print("\nUTC Offset Findings:")
    for offset, files in results.items():
        print(f"\nFound UTC offset: {offset}")
        print("Found in files:")
        for filepath in files:
            print(f" - {filepath}")

def main():
    if len(sys.argv) < 2:
        print("Usage: python3 utc_offset_extractor.py <search_path> [--convert-dlt]")
        print("\nOptions:")
        print("--convert-dlt   Convert .dlt files to .log format before processing")
        sys.exit(1)

    search_path = sys.argv[1]
    convert_dlt = '--convert-dlt' in sys.argv
    
    if not os.path.isdir(search_path):
        print(f"Error: {search_path} is not a valid directory")
        sys.exit(1)

    print(f"Scanning directory: {search_path}")
    if convert_dlt:
        print("DLT conversion enabled")
    
    results = scan_directory(search_path, convert_dlt)
    print_results(results)

if __name__ == '__main__':
    main()


Key features of this simplified version:

Focused Functionality:
Only extracts UTC offset values (like 28800) from "Current UTC offset:" lines
Removed all SELinux denial-related code
DLT Conversion:
Kept the DLT to log conversion capability
Enabled with --convert-dlt flag
Handles conversion errors gracefully
Improved Structure:
Better organized functions with single responsibilities
Clear separation between file processing and conversion logic
Proper error handling throughout
Output Format:
Clean reporting of found UTC offsets
Lists all files where each offset value was found


Usage examples:

# Basic scan (log files only)
python3 utc_offset_extractor.py /path/to/files

# Scan with DLT conversion
python3 utc_offset_extractor.py /path/to/files --convert-dlt

The script will:

Scan the specified directory and its subdirectories
Process all .log files (and .dlt files if conversion is enabled)
Extract all "Current UTC offset: X" values
Report each unique offset value and the files where it was found

===================================================================================================================================================================================

For following line-

IcAsilIceDetectionEES25::SubscribeUTCCustomerTimeProxy: Total UTC Customer Time offset: 480


Here's the modified code to extract the value "480" from lines matching the pattern 
IcAsilIceDetectionEES25::SubscribeUTCCustomerTimeProxy: Total UTC Customer Time offset: 480 in .dlt files:


import sys
import os
import re
import subprocess
import fnmatch

def extract_utc_offset(line):
    """Extract UTC Customer Time offset value from a line if present"""
    match = re.search(
        r'IcAsilIceDetectionEES25::SubscribeUTCCustomerTimeProxy:\s*Total UTC Customer Time offset:\s*(\d+)', 
        line
    )
    return match.group(1) if match else None

def process_file(filename, results):
    """Process a file to extract UTC Customer Time offsets"""
    try:
        with open(filename, encoding='utf8', errors='ignore') as f:
            for line in f:
                offset = extract_utc_offset(line)
                if offset:
                    results.setdefault(offset, set()).add(filename)
    except Exception as e:
        print(f"Error processing {filename}: {str(e)}")

def convert_dlt_to_log(dlt_file):
    """Convert DLT file to log format using dlt-viewer"""
    logfile = dlt_file + '.log'
    cmd = f"dlt-viewer -s -u -c {dlt_file} {logfile}"
    
    try:
        proc = subprocess.Popen(
            cmd.split(), 
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        proc.wait()
        return logfile if proc.returncode == 0 and os.path.exists(logfile) else None
    except Exception as e:
        print(f"Error converting {dlt_file}: {str(e)}")
        return None

def scan_directory(search_path, convert_dlt=False):
    """Scan directory for files and extract UTC Customer Time offsets"""
    results = {}
    
    for root, _, files in os.walk(search_path):
        for filename in files:
            filepath = os.path.join(root, filename)
            
            if fnmatch.fnmatch(filename, '*.log'):
                process_file(filepath, results)
            elif convert_dlt and fnmatch.fnmatch(filename, '*.dlt'):
                logfile = convert_dlt_to_log(filepath)
                if logfile:
                    process_file(logfile, results)
                    # Optional: Remove temporary log file after processing
                    # os.remove(logfile)
    
    return results

def print_results(results):
    """Print the collected UTC Customer Time offset results"""
    if not results:
        print("No UTC Customer Time offset values found in the scanned files.")
        return
    
    print("\nUTC Customer Time Offset Findings:")
    for offset, files in sorted(results.items(), key=lambda x: int(x[0])):
        print(f"\nFound UTC Customer Time offset: {offset} minutes")
        print("Found in files:")
        for filepath in sorted(files):
            print(f" - {filepath}")

def main():
    if len(sys.argv) < 2:
        print("Usage: python3 utc_customer_offset_extractor.py <search_path> [--convert-dlt]")
        print("\nOptions:")
        print("--convert-dlt   Convert .dlt files to .log format before processing")
        sys.exit(1)

    search_path = sys.argv[1]
    convert_dlt = '--convert-dlt' in sys.argv
    
    if not os.path.isdir(search_path):
        print(f"Error: {search_path} is not a valid directory")
        sys.exit(1)

    print(f"Scanning directory: {search_path}")
    if convert_dlt:
        print("DLT conversion enabled")
    
    results = scan_directory(search_path, convert_dlt)
    print_results(results)

if __name__ == '__main__':
    main()


Key changes made:

Pattern Matching:
Updated the regex pattern to specifically match
IcAsilIceDetectionEES25::SubscribeUTCCustomerTimeProxy: Total UTC Customer Time offset: 480
Changed the extraction function name to reflect the new purpose
Output Format:
Modified the results printing to indicate we're showing "UTC Customer Time offset"
Added "minutes" unit to the output for clarity
Sorted the results numerically by offset value
Error Handling:
Improved subprocess handling with universal_newlines=True
Added additional check for logfile existence after conversion
Code Clarity:
Updated all function docstrings and messages to reflect the new search pattern
Kept the same robust file processing and conversion framework
Usage remains the same:


# Basic scan (log files only)
python3 utc_customer_offset_extractor.py /path/to/files

# Scan with DLT conversion
python3 utc_customer_offset_extractor.py /path/to/files --convert-dlt


The script will now:

Specifically look for the UTC Customer Time offset pattern
Extract the numeric value (like 480)
Report all found values with their source files
Handle DLT conversion when requested
